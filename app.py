 import torch
from diffusers import StableDiffusionPipeline, ControlNetModel
from gtts import gTTS
import gradio as gr
import pandas as pd
import os
from PIL import Image
import numpy as np
import cv2
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
import clip
from huggingface_hub import hf_hub_download
from cachetools import TTLCache, cached
import json
from pathlib import Path

# --- CONFIGURATION ---
BASE_MODEL_ID = "runwayml/stable-diffusion-v1-5"
LORA_ADAPTER_PATH = "./biology_lora_adapter"Â 
DATA_FILE = "O level Biology.csv"
MODEL_CACHE_DIR = "./models"
CONTROLNET_ID = "lllyasviel/sd-controlnet-canny"
SAM_CHECKPOINT_URL = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"

# Creating model cache directory
os.makedirs(MODEL_CACHE_DIR, exist_ok=True)

def download_sam_checkpoint():
Â  Â  """Download SAM checkpoint if not exists"""
Â  Â  sam_path = os.path.join(MODEL_CACHE_DIR, "sam_vit_h_4b8939.pth")
Â  Â  if not os.path.exists(sam_path):
Â  Â  Â  Â  print("Downloading SAM checkpoint...")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  import urllib.request
Â  Â  Â  Â  Â  Â  urllib.request.urlretrieve(SAM_CHECKPOINT_URL, sam_path)
Â  Â  Â  Â  Â  Â  print("SAM checkpoint downloaded successfully!")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Failed to download SAM checkpoint: {e}")
Â  Â  Â  Â  Â  Â  return None
Â  Â  return sam_path

# Loading the dataset globally
try:
Â  Â  data = pd.read_csv(DATA_FILE)
except FileNotFoundError:
Â  Â  raise RuntimeError(f"Error: Could not find '{DATA_FILE}'. Please check the file name and location.")

# --- MODEL INITIALIZATION ---
class EnhancedPipeline:
Â  Â  def __init__(self):
Â  Â  Â  Â  self.device = "cuda" if torch.cuda.is_available() else "cpu"
Â  Â  Â  Â  self.dtype = torch.float16 if self.device == "cuda" else torch.float32
Â  Â  Â  Â Â 
Â  Â  Â  Â  if self.device == "cpu":
Â  Â  Â  Â  Â  Â  print("Warning: CUDA GPU not detected. Running on CPU may be extremely slow.")
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.pipe = self._init_stable_diffusion()
Â  Â  Â  Â  self.controlnet = self._init_controlnet()
Â  Â  Â  Â  self.clip_model = self._init_clip()
Â  Â  Â  Â  self.sam_generator = self._init_sam_automatic()Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  print("--- Enhanced Pipeline Loaded Successfully! ---")

Â  Â  def _init_stable_diffusion(self):
Â  Â  Â  Â  print("Loading Stable Diffusion...")
Â  Â  Â  Â  pipe = StableDiffusionPipeline.from_pretrained(
Â  Â  Â  Â  Â  Â  BASE_MODEL_ID,
Â  Â  Â  Â  Â  Â  torch_dtype=self.dtype,
Â  Â  Â  Â  Â  Â  safety_checker=None
Â  Â  Â  Â  )
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  pipe.load_lora_weights(LORA_ADAPTER_PATH)
Â  Â  Â  Â  Â  Â  print("LoRA weights loaded successfully.")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Failed to load LoRA weights: {e}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  pipe.to(self.device)
Â  Â  Â  Â  return pipe

Â  Â  def _init_controlnet(self):
Â  Â  Â  Â  print("Loading ControlNet...")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  controlnet = ControlNetModel.from_pretrained(
Â  Â  Â  Â  Â  Â  Â  Â  CONTROLNET_ID,
Â  Â  Â  Â  Â  Â  Â  Â  torch_dtype=self.dtype,
Â  Â  Â  Â  Â  Â  Â  Â  cache_dir=MODEL_CACHE_DIR
Â  Â  Â  Â  Â  Â  ).to(self.device)
Â  Â  Â  Â  Â  Â  print("ControlNet loaded successfully!")
Â  Â  Â  Â  Â  Â  return controlnet
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Error loading ControlNet: {e}")
Â  Â  Â  Â  Â  Â  raiseÂ 

Â  Â  def _init_clip(self):
Â  Â  Â  Â  print("Loading CLIP...")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  model, preprocess = clip.load("ViT-B/32", device=self.device, download_root=MODEL_CACHE_DIR)
Â  Â  Â  Â  Â  Â  print("CLIP loaded successfully!")
Â  Â  Â  Â  Â  Â  return {"model": model, "preprocess": preprocess}
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Error loading CLIP: {e}")
Â  Â  Â  Â  Â  Â  raise

Â  Â  def _init_sam_automatic(self):
Â  Â  Â  Â  print("Loading SAM Automatic Mask Generator...")
Â  Â  Â  Â  sam_path = download_sam_checkpoint()
Â  Â  Â  Â  if not sam_path:
Â  Â  Â  Â  Â  Â  raise RuntimeError("Failed to initialize SAM. Checkpoint download failed.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  sam = sam_model_registry["vit_h"](checkpoint=sam_path)
Â  Â  Â  Â  Â  Â  sam.to(device=self.device)
Â  Â  Â  Â  Â  Â  mask_generator = SamAutomaticMaskGenerator(sam)
Â  Â  Â  Â  Â  Â  print("SAM Automatic Mask Generator loaded successfully!")
Â  Â  Â  Â  Â  Â  return mask_generator
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"Error loading SAM: {e}")
Â  Â  Â  Â  Â  Â  raise

Â  Â  def generate_image(self, prompt, image_prompt):
Â  Â  Â  Â  full_prompt = f"{prompt}, {image_prompt}"
Â  Â  Â  Â  negative_prompt = "ugly, deformed, low quality, abstract, blurry, text"
Â  Â  Â  Â Â 
Â  Â  Â  Â  generator = torch.manual_seed(42)
Â  Â  Â  Â  init_image = np.array(self.pipe(full_prompt, num_inference_steps=1, generator=generator, negative_prompt=negative_prompt).images[0])
Â  Â  Â  Â  canny_image = cv2.Canny(init_image, 100, 200)
Â  Â  Â  Â  canny_image = Image.fromarray(canny_image)

Â  Â  Â  Â  image = self.pipe(
Â  Â  Â  Â  Â  Â  full_prompt,
Â  Â  Â  Â  Â  Â  image=canny_image,
Â  Â  Â  Â  Â  Â  controlnet=self.controlnet,
Â  Â  Â  Â  Â  Â  num_inference_steps=50,
Â  Â  Â  Â  Â  Â  guidance_scale=8.0,
Â  Â  Â  Â  Â  Â  generator=generator,
Â  Â  Â  Â  Â  Â  negative_prompt=negative_prompt
Â  Â  Â  Â  ).images[0]

Â  Â  Â  Â  return image

Â  Â  def validate_cultural_relevance(self, image, prompt):
Â  Â  Â  Â  image_input = self.clip_model["preprocess"](image).unsqueeze(0).to(self.device)
Â  Â  Â  Â  text_input = clip.tokenize([prompt]).to(self.device)

Â  Â  Â  Â  with torch.no_grad():
Â  Â  Â  Â  Â  Â  image_features = self.clip_model["model"].encode_image(image_input)
Â  Â  Â  Â  Â  Â  text_features = self.clip_model["model"].encode_text(text_input)
Â  Â  Â  Â  Â  Â  similarity = torch.cosine_similarity(image_features, text_features)

Â  Â  Â  Â  return similarity.item()

Â  Â  def refine_with_sam(self, image):
Â  Â  Â  Â  if isinstance(image, Image.Image):
Â  Â  Â  Â  Â  Â  image = np.array(image)

Â  Â  Â  Â  masks_list = self.sam_generator.generate(image)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not masks_list:
Â  Â  Â  Â  Â  Â  print("SAM found no segments to refine.")
Â  Â  Â  Â  Â  Â  return Image.fromarray(image)Â 

Â  Â  Â  Â  combined_mask = np.zeros(masks_list[0]['segmentation'].shape, dtype=bool)
Â  Â  Â  Â  for mask_data in masks_list:
Â  Â  Â  Â  Â  Â  combined_mask = np.logical_or(combined_mask, mask_data['segmentation'])
Â  Â  Â  Â Â 
Â  Â  Â  Â  refined_image = np.zeros_like(image)
Â  Â  Â  Â  refined_image[combined_mask] = image[combined_mask]
Â  Â  Â  Â Â 
Â  Â  Â  Â  return Image.fromarray(refined_image)

# Initializing the pipeline
pipeline = None
try:
Â  Â  pipeline = EnhancedPipeline()
except Exception as e:
Â  Â  print(f"CRITICAL STARTUP FAILURE: {e}")

# --- GENERATION LOGICÂ  ---
def generate_full_lesson(topic, level):
Â  Â Â 
Â  Â  if pipeline is None:
Â  Â  Â  Â  raise gr.Error("Pipeline failed to initialize. Check Colab logs for startup errors.")
Â  Â Â 
Â  Â  # 1.Find ALL matching rows for the topic
Â  Â  topic_data_df = data[(data['Topic'].str.strip().str.lower() == topic.strip().lower()) &Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (data['Level'] == level)]
Â  Â Â 
Â  Â  if topic_data_df.empty:
Â  Â  Â  Â  raise gr.Error("Topic not found. Please enter the exact topic name (e.g., 'Cells') and check the Level.")
Â  Â Â 
Â  Â  lesson_sequence = []
Â  Â  lesson_sequence.append((None, f"## ðŸ“š Lesson: {topic} ({level})"))
Â  Â Â 
Â  Â  # 2. Loop through each row and generate content
Â  Â  for index, row in topic_data_df.iterrows():
Â  Â  Â  Â  print(f"\n--- Generating Step {index+1}/{len(topic_data_df)}: {row['Instructional Step']} ---")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Get data from CSV row
Â  Â  Â  Â  image_prompt = row['Visual Prompt']
Â  Â  Â  Â  narration_script = row['Narration Script']
Â  Â  Â  Â  step_title = row['Instructional Step']
Â  Â  Â  Â Â 
Â  Â  Â  Â  #Â  Get Luganda and Swahili narrations ---
Â  Â  Â  Â  narration_lg = row['Luganda_Narration']
Â  Â  Â  Â  narration_sw = row['Swahili_Narration']

Â  Â  Â  Â  # 3. Generate and Validate Image
Â  Â  Â  Â  image = pipeline.generate_image(topic, image_prompt)
Â  Â  Â  Â Â 
Â  Â  Â  Â  relevance_score = pipeline.validate_cultural_relevance(image, image_prompt)
Â  Â  Â  Â  print(f"Cultural relevance score: {relevance_score}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if relevance_score < 0.25:Â 
Â  Â  Â  Â  Â  Â  print("Low cultural relevance - applying refinement")
Â  Â  Â  Â  Â  Â  image = pipeline.refine_with_sam(image)
Â  Â  Â  Â Â 
Â  Â  Â  Â  #Â  Saving image to a temporary file ---
Â  Â  Â  Â  image_path = f"image_step_{index}.png"
Â  Â  Â  Â  image.save(image_path)

Â  Â  Â  Â  # Generate all three Audio files
Â  Â  Â  Â  tts_en = gTTS(text=narration_script, lang='en')
Â  Â  Â  Â  audio_path_en = f"audio_step_{index}_en.mp3"
Â  Â  Â  Â  tts_en.save(audio_path_en)
Â  Â  Â  Â Â 
Â  Â  Â  Â  tts_lg = gTTS(text=narration_lg, lang='en')Â 
Â  Â  Â  Â  audio_path_lg = f"audio_step_{index}_lg.mp3"
Â  Â  Â  Â  tts_lg.save(audio_path_lg)

Â  Â  Â  Â  tts_sw = gTTS(text=narration_sw, lang='sw')
Â  Â  Â  Â  audio_path_sw = f"audio_step_{index}_sw.mp3"
Â  Â  Â  Â  tts_sw.save(audio_path_sw)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Add this step to the chatbot history
Â  Â  Â  Â  lesson_sequence.append((None, f"### {step_title}"))
Â  Â  Â  Â Â 
Â  Â  Â  Â  #Â  Add image file path to chat ---
Â  Â  Â  Â  lesson_sequence.append((None, (image_path,))) # Pass image path as a tuple
Â  Â  Â  Â Â 
Â  Â  Â  Â  lesson_sequence.append((None, narration_script)) # Display text
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- FIX 2: Add all three audio files to chat ---
Â  Â  Â  Â  lesson_sequence.append((None, (audio_path_en,))) # English
Â  Â  Â  Â  lesson_sequence.append((None, (audio_path_lg,))) # Luganda
Â  Â  Â  Â  lesson_sequence.append((None, (audio_path_sw,))) # Swahili
Â  Â  Â  Â Â 
Â  Â  Â  Â  lesson_sequence.append((None, "---")) # Add a separator

Â  Â  print("\n--- Full Lesson Generation Complete! ---")
Â  Â Â 
Â  Â  # Return the complete chat history
Â  Â  return lesson_sequence

# --- GRADIO INTERFACE ---
iface = gr.Interface(
Â  Â  fn=generate_full_lesson,
Â  Â  inputs=[
Â  Â  Â  Â  gr.Textbox(label="Enter Biology Topic (e.g., 'Cells')", value="Introduction to Biology"),
Â  Â  Â  Â  gr.Dropdown(label="Select Academic Level", choices=data['Level'].unique().tolist(), value="Senior 1")
Â  Â  ],
Â  Â  outputs=[
Â  Â  Â  Â  gr.Chatbot(label="Full Lesson Sequence", height=800)
Â  Â  ],
Â  Â  title="ðŸ”¬ Final Project: Full Lesson Generator",
Â  Â  description="Enter a topic (like 'Cells' or 'Introduction to Biology') to generate the complete, multi-step visual and audio lesson."
)

if __name__ == "__main__":
Â  Â  # The 'server_name="0.0.0.0"' ensures the app is accessible outside the container
Â  Â  iface.launch(server_name="0.0.0.0", server_port=7860)
